{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning Workflow Orchestration**\n",
    "\n",
    "Orchestration refers to the coordination and management of various tasks, resources, and processes involved in the end-to-end machine learning lifecycle. This includes:\n",
    "\n",
    "1. Data Preparation and Management\n",
    "2. Model Training\n",
    "3. Experimentation and Evaluaiton\n",
    "4. Model Deployment\n",
    "5. Monitor and Management\n",
    "6. Automation of repetitive tasks\n",
    "\n",
    "### **Introducing Prefect**  \n",
    "Prefect is an open-source orchestration and observability platform that empowers developers to build and scale resilient code quickly, turning their Python scripts into resilient, recurring workflows.\n",
    "\n",
    "Prefect streamlines the orchestration of machine learning workflows by providing a flexible, scalable, and reliable framework for building, deploying, and managing complex data pipelines with ease. It empowers data scientists and engineers to focus on building machine learning models and solving business problems while abstracting away the complexities of workflow management and execution.\n",
    "\n",
    "Prefect versions:\n",
    "- Prefect 1.x AKA Prefect Core\n",
    "- Prefect 2.x AKA Prefect Orion\n",
    "\n",
    "### **Why Prefect?**\n",
    "- Python based open source tool  \n",
    "- Manage ML Pipelines  \n",
    "- Schedule and Monitor the flow  \n",
    "- Gives observability into failures  \n",
    "- Native dask integration for scaling (Dask is used for parallel computing)\n",
    "\n",
    "\n",
    "### **Creating and activating a Virtual Environment**\n",
    "In order to install prefect, create a virtual environment:\n",
    "> `$ python -m venv .mlops_env`  \n",
    "\n",
    "Enter the Virtual Environment using below mentioned command:\n",
    "> `$ .mlops_env\\Scripts\\activate`\n",
    "\n",
    "***\n",
    "\n",
    "### **Installing Prefect 2.x**\n",
    "Now install Prefect:\n",
    "> `$ pip install prefect`  \n",
    "\n",
    "OR  if you have Prefect 1, upgrade to Prefect 2 using this command:  \n",
    "> `$ pip install -U prefect`  \n",
    "\n",
    "OR to install a specific version:  \n",
    "> `$ pip install prefect==2.4`  \n",
    "\n",
    "***\n",
    "\n",
    "### **Check Prefect Version**\n",
    "Check the prefect version:\n",
    "> `$ prefect version`\n",
    "\n",
    "***\n",
    "\n",
    "### **Running Prefect Dashboard (UI)**\n",
    "\n",
    "> `$ prefect server start`  \n",
    "\n",
    "```\n",
    " ___ ___ ___ ___ ___ ___ _____\n",
    "| _ \\ _ \\ __| __| __/ __|_   _|\n",
    "|  _/   / _|| _|| _| (__  | |\n",
    "|_| |_|_\\___|_| |___\\___| |_|\n",
    "\n",
    "Configure Prefect to communicate with the server with:\n",
    "\n",
    "    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "\n",
    "View the API reference documentation at http://127.0.0.1:4200/docs\n",
    "\n",
    "Check out the dashboard at http://127.0.0.1:4200\n",
    "```\n",
    "***\n",
    "\n",
    "**Note - In one of the earliest update of Prefect Orion, in Windows OS, if your path contains spaces, it will generate error (as mentioned below) when you try to run prefect orion. Sharing this so that you know what it is if you see it.**\n",
    "\n",
    "```\n",
    "___ ___ ___ ___ ___ ___ _____    ___  ___ ___ ___  _  _\n",
    "| _ \\ _ \\ __| __| __/ __|_   _|  / _ \\| _ \\_ _/ _ \\| \\| |\n",
    "|  _/   / _|| _|| _| (__  | |   | (_) |   /| | (_) | .` |\n",
    "|_| |_|_\\___|_| |___\\___| |_|    \\___/|_|_\\___\\___/|_|\\_|\n",
    "Configure Prefect to communicate with the server with:\n",
    "    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "View the API reference documentation at http://127.0.0.1:4200/docs\n",
    "Check out the dashboard at http://127.0.0.1:4200/\n",
    "Usage: uvicorn [OPTIONS] APP\n",
    "\n",
    "Try 'uvicorn --help' for help.\n",
    "\n",
    "Error: Got unexpected extra argument (prefect.orion.api.server:create_app)\n",
    "Orion stopped!\n",
    "```\n",
    "\n",
    "<img src=\"images/prefect_dashboard.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Refactoring the ML Workflow**\n",
    "\n",
    "If you've written the entire workflow code cell by cell in a Jupyter Notebook without explicitly defining functions, you may encounter difficulties when trying to visualize and monitor your flows in the Prefect dashboard.\n",
    "\n",
    "Prefect works best when workflows are organized into modular functions, with each function representing a task in your workflow. This allows Prefect to track task dependencies, visualize the workflow graph, and provide detailed execution logs and status updates in the dashboard.\n",
    "\n",
    "However, if you've written your workflow code directly in a Jupyter Notebook without defining functions, you can still use Prefect to run your workflows, but you may miss out on some of the dashboard's features and benefits.\n",
    "\n",
    "To address this, you can refactor your workflow code to extract each step into separate functions, and then import these functions into your notebook. This way, you can maintain the convenience of writing and experimenting with code in a notebook while also leveraging Prefect's capabilities for workflow orchestration and monitoring.\n",
    "\n",
    "Once you've refactored your code to use functions, you can run your flows as usual using Prefect's CLI or Python API, and you'll be able to visualize and monitor them in the Prefect dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_inputs_output(data, inputs, output):\n",
    "    \"\"\"\n",
    "    Split features and target variables.\n",
    "    \"\"\"\n",
    "    X = data[inputs]\n",
    "    y = data[output]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X, y, test_size=0.25, random_state=0):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets.\n",
    "    \"\"\"\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(raw_text):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = re.sub(\"[^a-zA-Z]|READ MORE\", \" \", raw_text)\n",
    "    sentence = sentence.lower()\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word for word in lemmatized_tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    cleaning the text data before hand.\n",
    "    \"\"\"\n",
    "    vect = CountVectorizer(preprocessor=clean, max_features=5000)\n",
    "    X_train_bow = vect.fit_transform(X_train)\n",
    "    X_test_bow = vect.transform(X_test)\n",
    "    return X_train_bow, X_test_bow, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train_bow, y_train, hyperparameters):\n",
    "    \"\"\"\n",
    "    Training the machine learning model.\n",
    "    \"\"\"\n",
    "    model = LogisticRegression(**hyperparameters)\n",
    "    model.fit(X_train_bow, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test):\n",
    "    \"\"\"\n",
    "    Evaluating the model.\n",
    "    \"\"\"\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    train_score = metrics.accuracy_score(y_train, y_train_pred)\n",
    "    test_score = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow():\n",
    "    DATA_PATH = 'product_reviews.csv'\n",
    "    INPUT = 'review_text'\n",
    "    OUTPUT = 'sentiment'\n",
    "    HYPERPARAMETERS = {'max_iter':1000,\n",
    "                       'C':1,\n",
    "                       'class_weight':'balanced',\n",
    "                       'l1_ratio':0.6,\n",
    "                       'penalty':'elasticnet',\n",
    "                       'solver':'saga'}\n",
    "    \n",
    "    # Load data\n",
    "    review = load_data(DATA_PATH)\n",
    "\n",
    "    # Identify Inputs and Output\n",
    "    X, y = split_inputs_output(review, INPUT, OUTPUT)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X, y)\n",
    "\n",
    "    # Preprocess the data\n",
    "    X_train_bow, X_test_bow, y_train, y_test = preprocess(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Build a model\n",
    "    model = train_model(X_train_bow, y_train, HYPERPARAMETERS)\n",
    "    \n",
    "    # Evaluation\n",
    "    train_score, test_score = evaluate_model(model, X_train_bow, y_train, X_test_bow, y_test)\n",
    "    \n",
    "    print(\"Train Score:\", train_score)\n",
    "    print(\"Test Score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9341681130671956\n",
      "Test Score: 0.9070286351803645\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
